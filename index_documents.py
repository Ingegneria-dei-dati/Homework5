'''import os
import glob
import hashlib
import re
import datetime
from bs4 import BeautifulSoup, element
from elasticsearch import Elasticsearch

# ============================================================
# 1. CONNESSIONE ELASTICSEARCH
# ============================================================

ES = Elasticsearch("http://localhost:9200")
INDEX_NAME = "research_articles_v2"

# ============================================================
# 2. MAPPING CON ANALYZER PERSONALIZZATI
# ============================================================

MAPPING = {
    "settings": {
        "analysis": {
            "analyzer": {
                "english_custom": {
                    "type": "english",
                    "stopwords": "_english_"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "title": {"type": "text", "analyzer": "english_custom"},
            "authors": {"type": "text", "analyzer": "standard"},
            "date": {
                "type": "date",
                "format": "yyyy-MM-dd||yyyy||dd MMM yyyy",
                "null_value": None
            },
            "abstract": {"type": "text", "analyzer": "english_custom"},
            "paragraphs": {"type": "text", "analyzer": "english_custom"},
            "content_full": {"type": "text", "analyzer": "english_custom"},
            "source": {"type": "keyword"},
            "file_path": {"type": "keyword"}
        }
    }
}

# ============================================================
# 3. CREA INDICE SE NON ESISTE
# ============================================================

def create_index():
    if ES.indices.exists(index=INDEX_NAME):
        print(f"Indice '{INDEX_NAME}' gi√† esistente")
    else:
        try:
            ES.indices.create(index=INDEX_NAME, body=MAPPING)
            print(f"Indice '{INDEX_NAME}' creato.")
        except Exception as e:
            print(f"ERRORE: Impossibile creare l'indice: {e}")

# ============================================================
# 4. ESTRAZIONE METADATI E PARAGRAFI DA HTML (Pulizia Autori Finale)
# ============================================================

def parse_html(filepath: str):
    """Estrae i metadati e il testo completo (suddiviso in paragrafi) dal file HTML."""
    try:
        with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
            html = f.read()
    except Exception as e:
        print(f"Errore nella lettura del file {filepath}: {e}")
        return None

    soup = BeautifulSoup(html, "html.parser")

    title, abstract, date = "", "", None
    authors = []

    # --- A. PARSING METADATI (Priorit√† Alta) ---

    title_tag = soup.find("meta", {"name": "citation_title"}) or soup.find("meta", {"name": "DC.title"}) or soup.title
    if title_tag:
        title = title_tag.get("content") if title_tag.name == 'meta' else title_tag.get_text(strip=True)
    if not title:
        h1 = soup.find("h1")
        if h1: title = h1.get_text(strip=True)

    for tag in soup.find_all("meta", {"name": ["citation_author", "DC.creator", "author"]}):
        author_content = tag.get("content")
        if author_content and author_content not in authors:
            authors.append(author_content)

    date_tag = soup.find("meta", {"name": "citation_publication_date"}) or soup.find("meta", {"name": "citation_date"})
    if date_tag:
        date = date_tag.get("content")

    abs_meta = soup.find("meta", {"name": "description"}) or soup.find("meta", {"name": "abstract"})
    if abs_meta:
        abstract = abs_meta.get("content").strip()

    # --- B. PARSING BLOCCHI SPECIFICI (Fallback Arxiv) ---

    if not authors:
        author_div = soup.find("div", {"class": "authors"})
        if author_div:
            author_links = author_div.find_all("a")
            authors.extend([link.get_text(strip=True) for link in author_links if link.get_text(strip=True) and link.get_text(strip=True) not in authors])
            if not authors:
                authors_text = author_div.get_text(separator=",", strip=True)
                authors.extend([a.strip() for a in authors_text.split(",") if a.strip()])

    if not abstract:
        abs_tag = soup.find("blockquote", {"id": "abstract"}) or soup.find("div", {"class": "abstract"})
        if abs_tag:
            abstract_text = abs_tag.get_text(strip=True)
            abstract = re.sub(r'^(Abstract:?\s*|abstract:?\s*)', '', abstract_text, flags=re.IGNORECASE).strip()

    if not date:
        date_tag = soup.find("span", {"class": "date"}) or soup.find("div", {"class": "date"})
        if date_tag:
            date = date_tag.get_text(strip=True)

    # -----------------------------------------------------------------
    # C. EURISTICA DEL CORPO (Pulizia e Gestione Data)
    # -----------------------------------------------------------------

    content_root = soup.find('div', id='content') or soup.find('article') or soup.body
    if content_root:
        full_text_start = content_root.get_text(" ", strip=True)[:3000]
        text_for_author_search = full_text_start

        # Pulizia del testo di partenza per isolare gli Autori
        if title:
            text_for_author_search = re.sub(re.escape(title), '', text_for_author_search, flags=re.IGNORECASE)
        if abstract:
            text_for_author_search = re.sub(re.escape(abstract), '', text_for_author_search, flags=re.IGNORECASE)
        text_for_author_search = re.sub(r'^(abstract|summary|introduction)\s*[:.]?', '', text_for_author_search, flags=re.IGNORECASE).strip()

        # 1. Fallback Autori (Euristica sul testo PULITO - Con lista minimalista)
        if not authors:
            author_candidates = re.findall(r'([A-Z][a-z]+\s+(?:[A-Z]\.\s*)?[A-Z][a-z]+)', text_for_author_search[:500])

            # Lista di parole chiave di affiliazione/localit√† da escludere
            INSTITUTION_KEYWORDS = ['University', 'Institute', 'Department', 'Science', 'Group', 'College',
                                    'Amazon', 'Seattle', 'Mannheim', 'Ohio', 'Web', 'State']

            CLEAN_AUTHORS_CANDIDATES = []
            for a in author_candidates:
                a_clean = a.strip()

                if re.search(r'^(Abstract|Introduction|Figure|Table|Appendix|Pipelines|Documents|Group)', a_clean, re.IGNORECASE):
                    continue

                if len(a_clean.split()) > 4:
                    continue

                if re.search(r'\d+', a_clean):
                    continue

                # Regola CRITICA: Esclusione per parole chiave istituzionali (pi√π efficace)
                if any(kw.lower() in a_clean.lower() for kw in INSTITUTION_KEYWORDS):
                    continue

                # Rimuovi ID/suffissi in minuscolo o singole lettere finali (es. 'alexander', 'r')
                a_final = re.sub(r'\s+[a-z]+$', '', a_clean)
                a_final = re.sub(r'\s+\w$', '', a_final)

                if a_final and len(a_final.split()) >= 2:
                    CLEAN_AUTHORS_CANDIDATES.append(a_final)

            authors.extend(list(dict.fromkeys(CLEAN_AUTHORS_CANDIDATES))[:15])

        # 2. Fallback Abstract
        if not abstract:
            abstract_match = re.search(r'(abstract|summary)\s*[:.]?\s*(.*?)(?=\s*(?:[I.]{2,}\s|\n\n|\n[A-Z]|$|References))',
                                       full_text_start, re.IGNORECASE | re.DOTALL)
            if abstract_match:
                abstract = abstract_match.group(2).strip()

        # 3. Fallback Data (Ricerca e Normalizzazione)
        if not date:
            date_match_full = re.search(
                r'(?i)(?:submitted\s+on|version\s+of|v\d+\])\s*(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4})',
                full_text_start[:1000]
            )

            if date_match_full:
                date_str = date_match_full.group(1).strip()
                try:
                    date_obj = datetime.datetime.strptime(date_str, '%d %b %Y')
                    date = date_obj.strftime('%Y-%m-%d')
                except ValueError:
                    date = date_str
            else:
                date_match_year = re.search(r'(\d{4})', full_text_start[:500])
                if date_match_year:
                    year = date_match_year.group(1)
                    if int(year) > 2000 and int(year) <= datetime.date.today().year:
                        date = year

                        # CRITICAL FIX: Imposta 'date' a None se non √® una stringa valida
    if date is not None and (isinstance(date, str) and (not date.strip() or '0000' in date)):
        date = None

    # RIMUOVE I DUPLICATI FINALI DAGLI AUTORI
    authors_str = ", ".join(list(dict.fromkeys(authors)))

    # -----------------------------------------------------------------
    # D. ESTRAZIONE TESTO COMPLETO E PARAGRAFI (Pulizia Paragrafi)
    # -----------------------------------------------------------------

    content_area = soup.find("body") or soup
    paragraphs_list = []
    abstract_lower = abstract.lower() if abstract else ""

    for p_tag in content_area.find_all("p"):
        paragraph_text = p_tag.get_text(" ", strip=True)

        if not paragraph_text or len(paragraph_text) < 20:
            continue

        # Filtro 1: Escludi l'Abstract
        if abstract and (paragraph_text.lower().startswith(abstract_lower) or abstract_lower.startswith(paragraph_text.lower())):
            continue

        # Filtro 2: Escludi le intestazioni
        if re.match(r'^\s*(\d+\.?\s+[A-Z][a-z]+|References|Appendix|I\.\s|II\.\s|III\.\s)', paragraph_text):
            continue

        # FILTRO 3: Rimuove i metadati di conversione e affiliazione
        paragraph_text = re.sub(r'\[.*?\]', '', paragraph_text, flags=re.DOTALL).strip()
        if re.search(r'(organization=|addressline=|city=|country=|@)', paragraph_text, re.IGNORECASE):
            paragraph_text = ""

        if paragraph_text:
            paragraphs_list.append(paragraph_text)

    paragraphs_content = " ".join(paragraphs_list)
    content_full = soup.get_text(" ", strip=True)

    if not paragraphs_content and content_full:
        paragraphs_content = content_full

    return {
        "title": title,
        "authors": authors_str,
        "date": date,
        "abstract": abstract,
        "paragraphs": paragraphs_content,
        "content_full": content_full,
        "file_path": filepath
    }

# ============================================================
# 5. INDICIZZAZIONE DOCUMENTI
# ============================================================

def index_document(doc):
    """Indicizza il singolo documento in Elasticsearch."""
    doc_id = hashlib.md5(doc["file_path"].encode()).hexdigest()
    try:
        ES.index(index=INDEX_NAME, id=doc_id, document=doc)
    except Exception as e:
        print(f"[ERRORE] Indicizzazione file {doc['file_path']} fallita: {e}")

def index_directory(path, source):
    """Processa e indicizza tutti i file HTML in una directory."""
    html_files = glob.glob(os.path.join(path, "*.html"))
    print(f"\nIndicizzazione cartella: {path}")
    print(f"File trovati: {len(html_files)}\n")

    indexed_count = 0
    for file in html_files:
        doc = parse_html(file)
        if not doc:
            print(f"[SKIP] Impossibile parsare: {file}")
            continue

        if not doc["title"] and not doc["paragraphs"]:
            print(f"[SKIP] Documento vuoto: {file}")
            continue

        doc["source"] = source
        index_document(doc)
        indexed_count += 1
        print(f"[OK] Indicizzato: {file}")

    print(f"Indicizzazione completata per {path}. Documenti indicizzati: {indexed_count}")

# ============================================================
# 6. FUNZIONE DI TEST ISOLATO PER DEBUG
# ============================================================

def test_single_file(filepath: str):
    """Esegue il parsing su un singolo file con output di debug."""
    if not os.path.exists(filepath):
        print(f"\nERRORE: File non trovato al percorso: {filepath}")
        return

    print(f"\n*** AVVIO TEST DI PARSING ISOLATO: {filepath} ***")
    doc = parse_html(filepath)

    if not doc:
        print("\nRISULTATO FINALE: Parsing fallito o file vuoto.")
        return

    print("\n--- RISULTATI ESTRATTI ---")
    print(f"Titolo: {doc.get('title')}")
    print(f"Autori: {doc.get('authors')}")
    print(f"Data: {doc.get('date')}")
    print(f"Abstract (Prime 150 char): {doc.get('abstract', '')[:150]}...")
    print(f"Paragrafi (Prime 150 char): {doc.get('paragraphs', '')[:150]}...")
    print("\n*** FINE TEST ***")


# ============================================================
# 7. MAIN
# ============================================================

def main():
    # --- Configurazione del test ---
    # MODIFICA QUESTO PERCORSO con il file problematico
    FILE_DA_DEBUGGARE = "html_corpus/arxiv_html/2301.06264v2.html"

    # 1. ESEGUI TEST ISOLATO
    if os.path.exists(FILE_DA_DEBUGGARE):
        print("ATTENZIONE: Eseguo prima il test isolato sul file problematico.")
        test_single_file(FILE_DA_DEBUGGARE)
        input("\nPremi INVIO per continuare con l'indicizzazione completa...")
    else:
        print("ATTENZIONE: File di debug non trovato. Proseguo con l'indicizzazione completa.")

    # 2. PROSEGUI CON L'INDICIZZAZIONE
    # Se devi ricreare l'indice per applicare le modifiche al mapping:
    # ES.indices.delete(index=INDEX_NAME, ignore=[400, 404])
    create_index()
    index_directory("html_corpus/arxiv_html", "arxiv")
    index_directory("html_corpus/pmc_html", "pubmed")

    print("\n\n*** Indicizzazione Completata ***")

    # -------------------------------
    # Controllo rapido dei documenti
    # -------------------------------
    print("\nVisualizzo alcuni documenti indicizzati:")
    res = ES.search(index=INDEX_NAME, body={
        "query": {"match_all": {}},
        "size": 20,
        "_source": ["title", "authors", "date", "abstract", "paragraphs"]
    })

    for hit in res['hits']['hits']:
        source = hit['_source']
        print("Titolo:", source.get('title'))
        print("Autori:", source.get('authors'))
        print("Data:", source.get('date'))
        print("Abstract:", source.get('abstract', '')[:100], "...")
        print("Paragrafi:", source.get('paragraphs', '')[:500], "...")
        print("-"*50)

if __name__ == "__main__":
    main()'''



import os
import glob
import hashlib
import re
import datetime
from bs4 import BeautifulSoup, element
from elasticsearch import Elasticsearch
from pathlib import Path
from extract_tables import extract_tables_from_html

# ============================================================
# 1. CONNESSIONE ELASTICSEARCH
# ============================================================

ES = Elasticsearch("http://localhost:9200")
INDEX_NAME = "research_articles_v2"

# ============================================================
# 2. MAPPING CON ANALYZER PERSONALIZZATI
# ============================================================

MAPPING = {
    "settings": {
        "analysis": {
            "analyzer": {
                "english_custom": {
                    "type": "english",
                    "stopwords": "_english_"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "title": {"type": "text", "analyzer": "english_custom"},
            "authors": {"type": "text", "analyzer": "standard"},
            # Formati di data accettati: "AAAA-MM-GG", "AAAA", "GG Mese AAAA"
            "date": {
                "type": "date",
                "format": "yyyy-MM-dd||yyyy||dd MMM yyyy",
                "null_value": None
            },
            "abstract": {"type": "text", "analyzer": "english_custom"},
            "paragraphs": {"type": "text", "analyzer": "english_custom"},
            "content_full": {"type": "text", "analyzer": "english_custom"},
            "source": {"type": "keyword"},
            "file_path": {"type": "keyword"}
        }
    }
}

# ============================================================
# 3. CREA INDICE (CON ELIMINAZIONE PRECEDENTE)
# ============================================================

def create_index(overwrite=False):
    """
    Crea l'indice. Se overwrite √® True, elimina l'indice se esiste gi√†.
    """
    if ES.indices.exists(index=INDEX_NAME):
        if overwrite:
            try:
                # üîÑ MODIFICA CHIAVE: Elimina l'indice esistente
                ES.indices.delete(index=INDEX_NAME, ignore=[400, 404])
                print(f"Indice '{INDEX_NAME}' eliminato con successo.")
            except Exception as e:
                print(f"ERRORE: Impossibile eliminare l'indice: {e}")
        else:
            print(f"Indice '{INDEX_NAME}' gi√† esistente. Saltando la creazione.")
            return

    try:
        ES.indices.create(index=INDEX_NAME, body=MAPPING)
        print(f"Indice '{INDEX_NAME}' creato.")
    except Exception as e:
        print(f"ERRORE: Impossibile creare l'indice: {e}")

# ============================================================
# 4. ESTRAZIONE METADATI E PARAGRAFI DA HTML
# ============================================================

def parse_html(filepath: str):
    """Estrae i metadati e il testo completo (suddiviso in paragrafi) dal file HTML."""
    try:
        with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
            html = f.read()
    except Exception as e:
        print(f"Errore nella lettura del file {filepath}: {e}")
        return None

    # Utilizziamo 'html.parser' che gestisce bene i tag mal formattati
    soup = BeautifulSoup(html, "html.parser")

    title, abstract, date = "", "", None
    authors = []

    # --- A. PARSING METADATI (Priorit√† Alta) ---

    title_tag = soup.find("meta", {"name": "citation_title"}) or soup.find("meta", {"name": "DC.title"}) or soup.title
    if title_tag:
        title = title_tag.get("content") if title_tag.name == 'meta' else title_tag.get_text(strip=True)
    if not title:
        h1 = soup.find("h1")
        if h1: title = h1.get_text(strip=True)

    for tag in soup.find_all("meta", {"name": ["citation_author", "DC.creator", "author"]}):
        author_content = tag.get("content")
        if author_content and author_content not in authors:
            authors.append(author_content)

    date_tag = soup.find("meta", {"name": "citation_publication_date"}) or soup.find("meta", {"name": "citation_date"})
    if date_tag:
        date = date_tag.get("content")

    abs_meta = soup.find("meta", {"name": "description"}) or soup.find("meta", {"name": "abstract"})
    if abs_meta:
        abstract = abs_meta.get("content").strip()

    # --- B. PARSING BLOCCHI SPECIFICI (Fallback Arxiv) ---

    if not authors:
        author_div = soup.find("div", {"class": "authors"})
        if author_div:
            author_links = author_div.find_all("a")
            authors.extend([link.get_text(strip=True) for link in author_links if link.get_text(strip=True) and link.get_text(strip=True) not in authors])
            if not authors:
                authors_text = author_div.get_text(separator=",", strip=True)
                authors.extend([a.strip() for a in authors_text.split(",") if a.strip()])

    if not abstract:
        abs_tag = soup.find("blockquote", {"id": "abstract"}) or soup.find("div", {"class": "abstract"})
        if abs_tag:
            abstract_text = abs_tag.get_text(strip=True)
            abstract = re.sub(r'^(Abstract:?\s*|abstract:?\s*)', '', abstract_text, flags=re.IGNORECASE).strip()

    if not date:
        date_tag = soup.find("span", {"class": "date"}) or soup.find("div", {"class": "date"})
        if date_tag:
            date = date_tag.get_text(strip=True)

    # -----------------------------------------------------------------
    # C. EURISTICA DEL CORPO (Pulizia e Gestione Data)
    # -----------------------------------------------------------------

    # Definisce il blocco di contenuto principale per l'analisi euristica
    content_root_for_heuristic = soup.find('div', id='content') or soup.find('article') or soup.body

    # Prende un campione di testo iniziale per la ricerca euristica (Autori/Abstract/Data)
    full_text_start = content_root_for_heuristic.get_text(" ", strip=True)[:3000] if content_root_for_heuristic else soup.get_text(" ", strip=True)[:3000]
    text_for_author_search = full_text_start

    # Pulizia del testo di partenza per isolare gli Autori
    if title:
        text_for_author_search = re.sub(re.escape(title), '', text_for_author_search, flags=re.IGNORECASE)
    if abstract:
        text_for_author_search = re.sub(re.escape(abstract), '', text_for_author_search, flags=re.IGNORECASE)
    text_for_author_search = re.sub(r'^(abstract|summary|introduction)\s*[:.]?', '', text_for_author_search, flags=re.IGNORECASE).strip()

    # 1. Fallback Autori (Euristica sul testo PULITO - Con lista minimalista)
    if not authors:
        # Cerca sequenze di Nomi e Cognomi
        author_candidates = re.findall(r'([A-Z][a-z]+\s+(?:[A-Z]\.\s*)?[A-Z][a-z]+)', text_for_author_search[:500])

        # Lista di parole chiave di affiliazione/localit√† da escludere
        INSTITUTION_KEYWORDS = ['University', 'Institute', 'Department', 'Science', 'Group', 'College',
                                'Amazon', 'Seattle', 'Mannheim', 'Ohio', 'Web', 'State']

        CLEAN_AUTHORS_CANDIDATES = []
        for a in author_candidates:
            a_clean = a.strip()

            if re.search(r'^(Abstract|Introduction|Figure|Table|Appendix|Pipelines|Documents|Group)', a_clean, re.IGNORECASE):
                continue

            if len(a_clean.split()) > 4:
                continue

            if re.search(r'\d+', a_clean):
                continue

            # Regola CRITICA: Esclusione per parole chiave istituzionali (pi√π efficace)
            if any(kw.lower() in a_clean.lower() for kw in INSTITUTION_KEYWORDS):
                continue

            # Rimuovi ID/suffissi in minuscolo o singole lettere finali (es. 'alexander', 'r')
            a_final = re.sub(r'\s+[a-z]+$', '', a_clean)
            a_final = re.sub(r'\s+\w$', '', a_final)

            if a_final and len(a_final.split()) >= 2:
                CLEAN_AUTHORS_CANDIDATES.append(a_final)

        authors.extend(list(dict.fromkeys(CLEAN_AUTHORS_CANDIDATES))[:15])

    # 2. Fallback Abstract
    if not abstract:
        abstract_match = re.search(r'(abstract|summary)\s*[:.]?\s*(.*?)(?=\s*(?:[I.]{2,}\s|\n\n|\n[A-Z]|$|References))',
                                   full_text_start, re.IGNORECASE | re.DOTALL)
        if abstract_match:
            abstract = abstract_match.group(2).strip()

    # 3. Fallback Data (Ricerca e Normalizzazione)
    if not date:
        date_match_full = re.search(
            r'(?i)(?:submitted\s+on|version\s+of|v\d+\])\s*(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4})',
            full_text_start[:1000]
        )

        if date_match_full:
            date_str = date_match_full.group(1).strip()
            try:
                date_obj = datetime.datetime.strptime(date_str, '%d %b %Y')
                date = date_obj.strftime('%Y-%m-%d')
            except ValueError:
                date = date_str
        else:
            date_match_year = re.search(r'(\d{4})', full_text_start[:500])
            if date_match_year:
                year = date_match_year.group(1)
                if int(year) > 2000 and int(year) <= datetime.date.today().year:
                    date = year

    # CRITICAL FIX: Imposta 'date' a None se non √® una stringa valida/vuota
    if date is not None and (isinstance(date, str) and (not date.strip() or '0000' in date)):
        date = None

    # RIMUOVE I DUPLICATI FINALI DAGLI AUTORI E PULISCE GLI SPAZI EXTRA
    authors = list(dict.fromkeys(authors))
    # üßπ Pulizia aggiuntiva: rimuove gli autori che sono troppo corti (probabili iniziali)
    authors = [a for a in authors if len(a) > 2 or (len(a.split()) > 1)]
    authors_str = ", ".join(authors)

    # -----------------------------------------------------------------
    # D. ESTRAZIONE TESTO COMPLETO E PARAGRAFI (Pulizia Paragrafi)
    # -----------------------------------------------------------------

    # Riferimento all'area di contenuto principale trovata in C, altrimenti usa body
    content_area = content_root_for_heuristic or soup.body
    paragraphs_list = []
    abstract_lower = abstract.lower() if abstract else ""

    for p_tag in content_area.find_all("p"):
        paragraph_text = p_tag.get_text(" ", strip=True)

        if not paragraph_text or len(paragraph_text) < 20:
            continue

        # Filtro 1: Escludi l'Abstract
        if abstract and (paragraph_text.lower().startswith(abstract_lower) or abstract_lower.startswith(paragraph_text.lower())):
            continue

        # Filtro 2: Escludi le intestazioni che sembrano numeri romani/riferimenti
        if re.match(r'^\s*(\d+\.?\s+[A-Z][a-z]+|References|Appendix|I\.\s|II\.\s|III\.\s)', paragraph_text):
            continue

        # FILTRO 3: Rimuove i metadati di conversione e affiliazione
        paragraph_text = re.sub(r'\[.*?\]', '', paragraph_text, flags=re.DOTALL).strip()
        if re.search(r'(organization=|addressline=|city=|country=|@)', paragraph_text, re.IGNORECASE):
            paragraph_text = ""

        if paragraph_text:
            paragraphs_list.append(paragraph_text)

    paragraphs_content = " ".join(paragraphs_list)

    # üìù Aggiornamento di content_full: si basa sull'intera pagina o sull'area principale
    content_full = soup.get_text(" ", strip=True)

    if not paragraphs_content and content_full:
        paragraphs_content = content_full # Fallback per i documenti senza <p>

    return {
        "title": title,
        "authors": authors_str,
        "date": date,
        "abstract": abstract,
        "paragraphs": paragraphs_content,
        "content_full": content_full,
        "file_path": filepath
    }

# ============================================================
# 5. INDICIZZAZIONE DOCUMENTI
# ============================================================

def index_document(doc):
    """Indicizza il singolo documento in Elasticsearch."""
    # Genera un ID basato sull'hash del percorso del file
    doc_id = hashlib.md5(doc["file_path"].encode()).hexdigest()
    try:
        # Utilizzo del parametro 'document' introdotto nelle versioni recenti della libreria
        ES.index(index=INDEX_NAME, id=doc_id, document=doc)
    except Exception as e:
        print(f"[ERRORE] Indicizzazione file {doc['file_path']} fallita: {e}")



#QUESTO NON ESTRAE LE TABELLE 
'''
def index_directory(path, source):
    """Processa e indicizza tutti i file HTML in una directory."""
    # Gestisce sia i percorsi Unix che Windows
    html_files = glob.glob(os.path.join(path, "*.html"))
    print(f"\nIndicizzazione cartella: {path}")
    print(f"File trovati: {len(html_files)}\n")

    indexed_count = 0
    for file in html_files:
        doc = parse_html(file)
        if not doc:
            print(f"[SKIP] Impossibile parsare: {file}")
            continue

        if not doc["title"] and not doc["paragraphs"]:
            print(f"[SKIP] Documento vuoto (mancano titolo e contenuto): {file}")
            continue

        doc["source"] = source
        index_document(doc)
        indexed_count += 1
        print(f"[OK] Indicizzato: {file}")

    print(f"Indicizzazione completata per {path}. Documenti indicizzati: {indexed_count}")
'''
def process_file(filepath: str, source: str):
    # 1) leggi HTML
    html = Path(filepath).read_text(encoding="utf-8", errors="ignore")

    # 2) parsalo con la tua parse_html esistente
    doc = parse_html(filepath)
    if not doc:
        return

    # 3) paper_id: per ora puoi usare l'hash o il nome file
    paper_id = Path(filepath).stem

    # 4) estrai tabelle
    tables = extract_tables_from_html(html, paper_id=paper_id)

    # 5) indicizza il documento principale (gi√† lo fai)
    doc["source"] = source
    index_document(doc)

    # 6) (opzionale) indicizza le tabelle in un indice "tables"
    # for t in tables:
    #     index_table_in_es(t)

def index_directory(path, source):
    """Processa e indicizza tutti i file HTML in una directory, incluse le tabelle."""
    
    html_files = glob.glob(os.path.join(path, "*.html"))
    print(f"\nIndicizzazione cartella: {path}")
    print(f"File trovati: {len(html_files)}\n")

    indexed_count = 0

    for file in html_files:
        try:
            process_file(file, source)   # üëà ORA richiama la funzione completa
            indexed_count += 1
            print(f"[OK] Processato: {file}")
        except Exception as e:
            print(f"[ERRORE] Impossibile processare {file}: {e}")

    print(f"Indicizzazione completata per {path}. Documenti indicizzati: {indexed_count}")


# ============================================================
# 6. FUNZIONE DI TEST ISOLATO PER DEBUG
# ============================================================

def test_single_file(filepath: str):
    """Esegue il parsing su un singolo file con output di debug."""
    if not os.path.exists(filepath):
        print(f"\nERRORE: File non trovato al percorso: {filepath}")
        return

    print(f"\n*** AVVIO TEST DI PARSING ISOLATO: {filepath} ***")
    doc = parse_html(filepath)

    if not doc:
        print("\nRISULTATO FINALE: Parsing fallito o file vuoto.")
        return

    print("\n--- RISULTATI ESTRATTI ---")
    print(f"Titolo: {doc.get('title')}")
    print(f"Autori: {doc.get('authors')}")
    print(f"Data: {doc.get('date')}")
    print(f"Abstract (Prime 150 char): {doc.get('abstract', '')[:150]}...")
    print(f"Paragrafi (Prime 150 char): {doc.get('paragraphs', '')[:150]}...")
    print("\n*** FINE TEST ***")


# ============================================================
# 7. MAIN
# ============================================================

def main():
    # --- Configurazione del test ---
    # MODIFICA QUESTO PERCORSO con il file problematico
    FILE_DA_DEBUGGARE = "html_corpus/arxiv_html/2301.06264v2.html"

    # ‚öôÔ∏è IMPOSTAZIONE CHIAVE: Forzare la ricreazione dell'indice ad ogni run
    FORZA_RICREAZIONE_INDICE = True

    # 1. ESEGUI TEST ISOLATO
    if os.path.exists(FILE_DA_DEBUGGARE):
        print("ATTENZIONE: Eseguo prima il test isolato sul file problematico.")
        test_single_file(FILE_DA_DEBUGGARE)
        input("\nPremi INVIO per continuare con l'indicizzazione completa...")
    else:
        print("ATTENZIONE: File di debug non trovato. Proseguo con l'indicizzazione completa.")

    # 2. PROSEGUI CON L'INDICIZZAZIONE
    create_index(overwrite=FORZA_RICREAZIONE_INDICE)

    # 3. INDICIZZA LE DIRECTORY
    index_directory("html_corpus/arxiv_html", "arxiv")
    index_directory("html_corpus/pmc_html", "pubmed")

    print("\n\n*** Indicizzazione Completata ***")

    # -------------------------------
    # Controllo rapido dei documenti
    # -------------------------------
    print("\nVisualizzo alcuni documenti indicizzati:")
    # Aggiungo un try-except per gestire l'assenza di sicurezza di ES
    try:
        res = ES.search(index=INDEX_NAME, body={
            "query": {"match_all": {}},
            "size": 20,
            "_source": ["title", "authors", "date", "abstract", "paragraphs"]
        })

        for hit in res['hits']['hits']:
            source = hit['_source']
            print("Titolo:", source.get('title'))
            print("Autori:", source.get('authors'))
            print("Data:", source.get('date'))
            print("Abstract:", source.get('abstract', '')[:100], "...")
            print("Paragrafi:", source.get('paragraphs', '')[:500], "...")
            print("-"*50)
    except Exception as e:
        print(f"Errore durante la ricerca di verifica: {e}")

if __name__ == "__main__":
    main()